<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<title>ORB Core Status</title>
<!-- $Id$ -->
</head>
<body text="#000000" bgcolor="#FFFFFF">

<!-- *********************************** -->
<a name="toc"><h2>Table of Contents</h2></a>
<ul>
  <li><a href="#current">Current Status</a>
  <li><a href="#issues">Known Issues</a>
  <li><a href="#ongoing">Ongoing work</a>
  <li><a href="#future">Future Work</a>
  <li><a href="#completed">Completed Work</a>
</ul>
<!-- *********************************** -->

<hr>
<center>
<h1> <a name="orb"></a>ORB &amp; ORB Core</h1>

Point of contact: <a href="mailto: bala@cs.wustl.edu">Balchandran Natarajan</a>

<p>Last Update: $Date$</p>
</center>

<!-- ============================================================ -->
<a name="current"><h3>Current status:</h3></a>

<ul>

  <li><p>We are compliant with OMG's CORBA 2.6 specification. </p>
  </li>

</ul>

<!-- --------------------- -->
<a href="#toc">Back to TOC</a>
<!-- --------------------- -->

<!-- ============================================================ -->
<a name="issues"><h3>Known issues:</h3></a>

<ul>
  <li><p> TAO's support for wstring as per GIOP 1.2 rules are not
      perfect. There have been reported interoperability
      problems. <p></li>

  <li><p> The multi-profile parsing code in TAO hasnt been tested well
      and we have been able to identify some issues with it. The
      details are documented in our  <A
      HREF="http://deuce.doc.wustl.edu/bugzilla/index.cgi">bug tracking
	    system</A> under id's  <A
      HREF="http://deuce.doc.wustl.edu/bugzilla/show_bug.cgi?id=1237">1237</A>, <A
      HREF="http://deuce.doc.wustl.edu/bugzilla/show_bug.cgi?id=1238">
      1238</A>and<A
      HREF="http://deuce.doc.wustl.edu/bugzilla/show_bug.cgi?id=1239">1239</A> <p></li>

  <li><p> Applications using the distributed callback paradigm could
      see crashes if they use multiple nested upcalls. The nested
      upcalls sets of a stack growth and crashes when it runs out of
      stack space. This happens when an upcall as a part of the
      request leads to another request. The client thread waiting for
      the reply in the reactor could in turn process yet another
      request to build up the stack. The unbounded growth on the stack
      leads to crash failures. <p>
  </li>

      
</ul>
<!-- --------------------- -->
<a href="#toc">Back to TOC</a>
<!-- --------------------- -->

<!-- ============================================================ -->
<a name="ongoing"><h3>Ongoing Work:</h3></a>

<ul>
  <li><p>Support new concurrency models, e.g., Thread-per-Request,
      etc..</p>
  </li>
  <li><p>Improvement to connection cache on the acceptor side so that
      it will purge old connections when there are no more slots or
      sockets are available. The connection cache does good job of
      purging things on the connector side</p> 
  </li>

  <li><p> Prevent unbounded stack growth that leads to a failure in
      situations explained in <a href="#issues">known issues</a>.</p>
  </li>

</ul>
<!-- --------------------- -->
<a href="#toc">Back to TOC</a>
<!-- --------------------- -->

<!-- ============================================================ -->
<a name="future"><h3>Future work:</h3></a>

<ul>
  <li>Performance bottlenecks:</li>
      <ol>
        <li><p> Need to identify performance bottlenecks. Looks like
        TAO has slowed down a bit over the past two betas. Need to
        identify the bottlenecks before improving the performance.</p>
        </li>
        
        <li><p> Its a known fact that thread-per-connection is faster
        than the reactive model, even for single threaded cases. Need
        to identify portions of the reactor that slows down things and
        fix them or create a fast path in the ORB just using plain
        vanilla select for the server.</p>
        </li>
        
        <li><p> In the output data path, the CDR creates the message
        block and all other associted things on TSS. If we could
        change the CDR to first create them on the stack and then move
        the data to TSS we could achieve better performance for small
        messages.
      </ol>


  <li>Features:</li>
      <ol>
        <li><p>Implement DP-CORBA specification</p></li>
        
        <li><p>Implement TII specification.</p></li>
      </ol>

</ul>
<!-- --------------------- -->
<a href="#toc">Back to TOC</a>
<!-- --------------------- -->

<!-- ============================================================ -->
<a name="completed"><h3>Completed Work:</h3></a>

<!-- ************************************************************ -->
<!-- Please make sure you append new items at the end of the list -->
<!-- ************************************************************ -->
<ul>
    <li><p> Implemented a connection timeout policy in the ORB. This
        is useful for applications to set a timeout value associated
        with connection establishment. The name of the policy is
        <code>TAO::CONNECTION_TIMEOUT_POLICY_TYPE</code> and is
        proprietary to TAO. It comes as a prt of the TAO_Messaging
        library. Please see
        <code>$TAO_ROOT/tests/Connection_Timeout</code> for an example
        of how to use this feature.<p>
    </li>
        
    <li><p> Implemented non-blocking connects with three different
        strategies for connection establishment. The details of the
        strategies and the related documentation can be found <A
        href="../Options.html">here</a>.</p>
    </li>

    <li><p> Deprecated the option -ORBConnectionCachingStrategy. The
    option -ORBConnectionPurgingStrategy fills in the role of the
    deprecated strategy.</p>
    </li>
    
    <li><p> Deprecated the option -ORBResources.</p></li>

    <li><p> The connectors have been moved to the lanes.</p> </li>
    
    <li><p> Fixed stack recursion problems that occurs, when a thread
        flushing a reply by entering the reactor starts processing
        incoming requests.</p>
    </li>

    <li><p> Fixed connection handling and connection closing problems,
        when a write call to a remote host fails. </p>
    </li>

    <li> <p> Moved the allocators used for allocating memory for the
        incoming data path to the lanes. Only allocators on the
        incoming data path are moved into the lanes. Allocators on the
        outgoing data path are still on a per-orb basis. Since the
        outgoing data path uses memory from the TSS, moving them to
        the lanes is not necessary.
        </p>
    </li>
    <li><p> Added an option, -ORBSingleReadOptimization that allows
        users to turn on or turn off the single read
        optimization. This is useful for RTCORBA server threads to
        avoid priority inversions. Buffering messages and sending
        notifications to the reactor, that does not know or honour
        priority could lead to priority inversions. 
        </p>
    </li>

    <li><p> Several changes were made to utilize the new memory management mechanisms
	  of Event Handlers in TAO.  Previously, both the connection handlers and
	  the transports had their own reference counting mechanism and
	  independent lifetimes.  The reference counting was inadequate
	  since it was not originated at the Reactor and the independent
	  lifetimes unnecessarily completed protocol specific transport
	  code.  <p>

	  The changes couple the lifetime of the connection handler with
	  that of the transport and utilizes the reference count in the
	  base ACE_Event_Handler class.  This allowed us to remove several
	  hacks throughout TAO code which were designed to prevent (or
	  delay) crashes.  Several extra checks and unnecessary locks were
	  also removed. <p>

	  All of the following protocols were updated: DIOP, IIOP,
          SCIOP, SHMIOP, SSLIOP, UIOP, UIPMC. </p>
    </li>
    
    <li><p> Support for the ORB methods create_value_tc, create_component_tc,
            create_event_tc, create_home_tc and create_recursive_tc added. </p>
    </li>

</ul>
<!-- --------------------- -->
<a href="#toc">Back to TOC</a>
<!-- --------------------- -->

</BODY>
</HTML>
